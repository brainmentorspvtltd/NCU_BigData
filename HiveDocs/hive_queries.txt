
--Create table using ARRAY, MAP, STRUCT, and Composite data type
CREATE external TABLE employee_2 (
  name string comment 'name of employee',
  work_place ARRAY<string>,
  gender_age STRUCT<gender:string,age:int>,
  skills_score MAP<string,int>,
  depart_title MAP<STRING,ARRAY<STRING>>
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':'
STORED AS TEXTFILE
location 'hdfs://localhost:9000/hive/emp_series';

--Verify tables creations run in Beeline
!table employee

--Load data
LOAD DATA INPATH 'hdfs://localhost:9000/input_data/emp.txt' OVERWRITE INTO TABLE employee_2;


--Query the whole table
SELECT * FROM employee;

--Query the ARRAY in the table
SELECT work_place FROM employee_2;

SELECT work_place[0] AS col_1, work_place[1] AS col_2, work_place[2] AS col_3 FROM employee_2;

--Query the STRUCT in the table
SELECT gender_age FROM employee_2;

SELECT gender_age.gender, gender_age.age FROM employee_2;

--Query the MAP in the table
SELECT skills_score FROM employee;

SELECT name, skills_score['DB'] AS DB, 
skills_score['Perl'] AS Perl, skills_score['Python'] AS Python, 
skills_score['Sales'] as Sales, skills_score['HR'] as HR FROM employee;

SELECT depart_title FROM employee;

SELECT name, depart_title['Product'] AS Product, depart_title['Test'] AS Test,
depart_title['COE'] AS COE, depart_title['Sales'] AS Sales  
FROM employee;

SELECT name, 
depart_title['Product'][0] AS product_col0, 
depart_title['Test'][0] AS test_col0 
FROM employee;



--Create database without checking if the database already exists.
CREATE DATABASE mydatabase;

–-Create database and checking if the database already exists.
CREATE DATABASE IF NOT EXISTS mydatabase;

--Create database with location, comments, and metadata information
CREATE DATABASE IF NOT EXISTS myhivebook
COMMENT 'hive database demo'
LOCATION 'gs://pig-bucket1/"
WITH DBPROPERTIES ('creator'='X','date'='2020-01-01');

--Show and describe database with wildcards
SHOW DATABASES;
SHOW DATABASES LIKE 'my.*';
DESCRIBE DATABASE default;

--Use the database
USE myhivebook;

--Show current database
SELECT current_database();

--Drop the empty database.
DROP DATABASE IF EXISTS myhivebook;

--Drop database with CASCADE
DROP DATABASE IF EXISTS myhivebook CASCADE;

--metadata about database could not be changed.
ALTER DATABASE myhivebook SET DBPROPERTIES ('edited-by' = 'Dayong');

ALTER DATABASE myhivebook SET OWNER user dayongd;

--Chapter 3 Code - Hive Table DDL

--Create internal table and load the data
CREATE TABLE IF NOT EXISTS employee_internal (
  name string,
  work_place ARRAY<string>,
  gender_age STRUCT<gender:string,age:int>,
  skills_score MAP<string,int>,
  depart_title MAP<STRING,ARRAY<STRING>>
)
COMMENT 'This is an internal table'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':'
STORED AS TEXTFILE;

LOAD DATA INPATH 'gs://pig-bucket1/emp.txt/emp.txt' OVERWRITE INTO TABLE employee_internal;

--Create external table and load the data
CREATE EXTERNAL TABLE IF NOT EXISTS employee_external (
   name string,
   work_place ARRAY<string>,
   gender_age STRUCT<gender:string,age:int>,
   skills_score MAP<string,int>,
   depart_title MAP<STRING,ARRAY<STRING>>
)
COMMENT 'This is an external table'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':'
STORED AS TEXTFILE
LOCATION 'gs://pig-bucket1/emp.txt';

LOAD DATA INPATH '/tmp/hivedemo/data/employee.txt' OVERWRITE INTO TABLE employee_external;

--Create Table With Data - CREATE TABLE AS SELECT (CTAS)
CREATE TABLE ctas_employee AS SELECT * FROM employee_external;

--Temporary tables
CREATE TEMPORARY TABLE IF NOT EXISTS tmp_emp1 (
name string,
work_place ARRAY<string>,
gender_age STRUCT<gender:string,age:int>,
skills_score MAP<string,int>,
depart_title MAP<STRING,ARRAY<STRING>>
); 

CREATE TEMPORARY TABLE tmp_emp2 AS SELECT * FROM tmp_emp1;

CREATE TEMPORARY TABLE tmp_emp3 LIKE tmp_emp1;

--Create Table As SELECT (CTAS) with Common Table Expression (CTE) 
CREATE TABLE cte_employee AS
WITH r1 AS (SELECT name FROM r2 WHERE name = 'Michael'),
r2 AS (SELECT name FROM employee WHERE gender_age.gender= 'Male'),
r3 AS (SELECT name FROM employee WHERE gender_age.gender= 'Female')
SELECT * FROM r1 UNION ALL select * FROM r3;

SELECT * FROM cte_employee;

--Create Table Without Data - TWO ways 
--With CTAS
CREATE TABLE empty_ctas_employee AS SELECT * FROM employee_internal WHERE 1=2;

--With LIKE
CREATE TABLE empty_like_employee LIKE employee_internal;

--Check row count for both tables
SELECT COUNT(*) AS row_cnt FROM empty_ctas_employee;
SELECT COUNT(*) AS row_cnt FROM empty_like_employee;

--Show tables
SHOW TABLES;
SHOW TABLES '*sam*';
SHOW TABLES '*sam|lily*';
SHOW TABLE EXTENDED LIKE 'employee_int*';

--Show columns
SHOW COLUMNS IN employee_internal;
DESC employee_internal;

--Show DDL and property
SHOW CREATE TABLE employee_internal;
SHOW TBLPROPERTIES employee_internal;

--Drop table 
DROP TABLE IF EXISTS empty_ctas_employee;

DROP TABLE IF EXISTS empty_like_employee;

--Truncate table
SELECT * FROM cte_employee;

TRUNCATE TABLE cte_employee;

SELECT * FROM cte_employee;

--Alter table statements
--Alter table name
ALTER TABLE cte_employee RENAME TO cte_employee_backup;

--Alter table properties, such as comments
ALTER TABLE c_employee SET TBLPROPERTIES ('comment' = 'New comments');

--Alter table delimiter through SerDe properties
ALTER TABLE employee_internal SET SERDEPROPERTIES ('field.delim' = '$');

--Alter Table File Format
ALTER TABLE c_employee SET FILEFORMAT RCFILE;

--Alter Table Location
ALTER TABLE c_employee SET LOCATION 'hdfs://localhost:9000/user/dayongd/employee'; 

--Alter Table Location
ALTER TABLE c_employee ENABLE NO_DROP; 
ALTER TABLE c_employee DISABLE NO_DROP; 
ALTER TABLE c_employee ENABLE OFFLINE;
ALTER TABLE c_employee DISABLE OFFLINE;

--Alter Table Concatenate to merge small files into larger files
--convert to the file format supported
ALTER TABLE c_employee SET FILEFORMAT ORC;
 
--concatenate files
ALTER TABLE c_employee CONCATENATE;

--convert to the regular file format
ALTER TABLE c_employee SET FILEFORMAT TEXTFILE;


--Alter columns
--Change column type - before changes
DESC employee_internal; 

--Change column type
ALTER TABLE employee_internal CHANGE name employee_name string AFTER gender_age;

--Verify the changes 
DESC employee_internal; 

--Change column type
ALTER TABLE employee_internal CHANGE employee_name name string FIRST;

--Verify the changes 
DESC employee_internal; 

--Add/Replace Columns-before add
DESC c_employee;      

--Add columns to the table
ALTER TABLE c_employee ADD COLUMNS (work string);

--Verify the added columns
DESC c_employee;      

--Replace all columns
ALTER TABLE c_employee REPLACE COLUMNS (name string);

--Verify the replaced all columns
DESC c_employee;   

--Chapter 3 Code - Hive Partition and Buckets DDL

--Create partition table DDL
CREATE TABLE employee_partitioned
(
  name string,
  work_place ARRAY<string>,
  gender_age STRUCT<gender:string,age:int>,
  skills_score MAP<string,int>,
  depart_title MAP<STRING,ARRAY<STRING>>
)
PARTITIONED BY (Year INT, Month INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':';

--Check partition table structure
DESC employee_partitioned;

--Show partitions
SHOW PARTITIONS employee_partitioned;

--Add multiple partitions
ALTER TABLE employee_partitioned ADD 
PARTITION (year=2018, month=11)        
PARTITION (year=2018, month=12);

SHOW PARTITIONS employee_partitioned;

--Drop partitions
ALTER TABLE employee_partitioned DROP PARTITION (year=2018, month=11);

ALTER TABLE employee_partitioned DROP IF EXISTS PARTITION (year=2017); -- Drop all partitions in 2017

ALTER TABLE employee_partitioned DROP IF EXISTS PARTITION (month=9);

SHOW PARTITIONS employee_partitioned;

--Rename partitions
ALTER TABLE employee_partitioned PARTITION (year=2018, month=12) RENAME TO PARTITION (year=2018,month=10);

SHOW PARTITIONS employee_partitioned;

--ALTER TABLE employee_partitioned PARTITION (year=2018) RENAME TO PARTITION (year=2017); --Failed, must specify all partitions

--Load data to the partition
LOAD DATA INPATH 'hdfs://localhost:9000/input_data/emp.txt' 
OVERWRITE INTO TABLE employee_partitioned
PARTITION (year=2018, month=12);

--Verify data loaded
SELECT name, year, month FROM employee_partitioned;

--Partition table add columns
ALTER TABLE employee_partitioned ADD COLUMNS (work string) CASCADE;

--Change data type for partition columns
ALTER TABLE employee_partitioned PARTITION COLUMN(year string);
--Verify the changes
DESC employee_partitioned;

ALTER TABLE employee_partitioned PARTITION (year=2018) SET FILEFORMAT ORC;
ALTER TABLE employee_partitioned PARTITION (year=2018) SET LOCATION '/tmp/data';
ALTER TABLE employee_partitioned PARTITION (year=2018) ENABLE NO_DROP;
ALTER TABLE employee_partitioned PARTITION (year=2018) ENABLE OFFLINE;
ALTER TABLE employee_partitioned PARTITION (year=2018) DISABLE NO_DROP;
ALTER TABLE employee_partitioned PARTITION (year=2018) DISABLE OFFLINE;
ALTER TABLE employee_partitioned PARTITION (year=2018) CONCATENATE;

--Create a table with bucketing
--Prepare data for backet tables
CREATE TABLE employee_id                         
(
  name string,
  employee_id int,
  work_place ARRAY<string>,
  gender_age STRUCT<gender:string,age:int>,
  skills_score MAP<string,int>,
  depart_title MAP<STRING,ARRAY<STRING>>
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':';

LOAD DATA INPATH '/tmp/hivedemo/data/employee_id.txt' OVERWRITE INTO TABLE employee_id;

--Create the bucket table
CREATE TABLE employee_id_buckets                         
(
  name string,
  employee_id int,
  work_place ARRAY<string>,
  gender_age STRUCT<gender:string,age:int>,
  skills_score MAP<string,int>,
  depart_title MAP<STRING,ARRAY<STRING>>
)
CLUSTERED BY (employee_id) INTO 2 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':';

set map.reduce.tasks = 2;

set hive.enforce.bucketing = true;

INSERT OVERWRITE TABLE employee_id_buckets SELECT * FROM employee_id;

--Chapter 3 Code - Hive View DDL

--Create Hive view
CREATE VIEW employee_skills
AS
SELECT name, skills_score['DB'] AS DB,
skills_score['Perl'] AS Perl, skills_score['Python'] AS Python,
skills_score['Sales'] as Sales, skills_score['HR'] as HR 
FROM employee;

--Show views
SHOW VIEWS;
SHOW VIEWS 'employee_*';
DESC FORMATTED employee_skills;
SHOW CREATE TABLE employee_skills;

--Alter views properties
ALTER VIEW employee_skills SET TBLPROPERTIES ('comment' = 'This is a view');

--Redefine views
ALTER VIEW employee_skills AS SELECT * from employee ;

--Drop views
DROP VIEW employee_skills; 

--Lateralview
SELECT name, workplace FROM employee_internal 
LATERAL VIEW explode(work_place) wp as workplace;

SELECT name, workplace FROM employee_internal
LATERAL VIEW explode(split(null, ',')) wp AS workplace;

SELECT name, workplace FROM employee_internal
LATERAL VIEW OUTER explode(split(null, ',')) wp AS workplace;

--Apache Hive Essentials 
--Chapter 4 Code - Hive SELECT, JOIN, and UNION
--Query all columns in the table
SELECT * FROM employee;

--Select only one column
SELECT name FROM employee;

--List columns meet java regular expression
SET hive.support.quoted.identifiers = none;
SELECT `^work.*` FROM employee;

--Select unique rows
SELECT DISTINCT name, work_place FROM employee;

--Select with UDF, IF, and CASE WHEN
SELECT 
CASE WHEN gender_age.gender = 'Female' THEN 'Ms.'
ELSE 'Mr.' END as title,
name, 
IF(array_contains(work_place, 'New York'), 'US', 'CA') as country
FROM employee;

--Nest SELECT after the FROM
SELECT name, gender_age.gender AS gender
FROM(
SELECT * FROM employee
WHERE gender_age.gender = 'Male'
) t1;

--Nest SELECT using CTE
WITH t1 AS (
SELECT * FROM employee
WHERE gender_age.gender = 'Male')
SELECT name, gender_age.gender AS gender FROM t1;

--Select with expression
SELECT concat('1','+','3','=',cast((1 + 3) as string)) as res;

--Filter data with limit
SELECT name FROM employee LIMIT 2;

--Filter with Where
SELECT name, work_place FROM employee WHERE name = 'Michael';

--Filter with Where and Limit
SELECT name, work_place FROM employee WHERE name = 'Michael' LIMIT 1;

--Filter with in
SELECT name FROM employee WHERE gender_age.age in (27, 30);

--In for multiple columns Works after v2.1.0
SELECT 
name, gender_age
FROM employee 
WHERE (gender_age.gender , gender_age.age) IN (('Female', 27), ('Male', 27 + 3));

--Subquery in
SELECT name, gender_age.gender AS gender
FROM employee a
WHERE a.name IN (SELECT name FROM employee WHERE gender_age.gender = 'Male');

--Subquery exists
SELECT name, gender_age.gender AS gender
FROM employee a
WHERE EXISTS
(SELECT * FROM employee b WHERE a.gender_age.gender = b.gender_age.gender AND b.gender_age.gender = 'Male');
 
--Prepare another table for join and load data
CREATE TABLE IF NOT EXISTS employee_hr
(
  name string,
  employee_id int,
  sin_number string,
  start_date date
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA INPATH '/tmp/hivedemo/data/employee_hr.txt' OVERWRITE INTO TABLE employee_hr;

--Equal JOIN between two tables
SELECT emp.name, emph.sin_number
FROM employee emp
JOIN employee_hr emph ON emp.name = emph.name;

--Unequal join returns more rows
SELECT 
emp.name, emph.sin_number
FROM employee emp 
JOIN employee_hr emph ON emp.name != emph.name;

--Join with complex expression - conditional join
SELECT 
emp.name, emph.sin_number
FROM employee emp
JOIN employee_hr emph ON 
IF(emp.name = 'Will', '1', emp.name) = CASE WHEN emph.name = 'Will' THEN '0' ELSE emph.name END;

-- Use Where to limit the output of join
SELECT 
emp.name, emph.sin_number
FROM employee emp
JOIN employee_hr emph ON emp.name = emph.name
WHERE
emp.name = 'Will';

--JOIN between more tables
SELECT emp.name, empi.employee_id, emph.sin_number
FROM employee emp
JOIN employee_hr emph ON emp.name = emph.name
JOIN employee_id empi ON emp.name = empi.name;

--Self join is used when the data in the table has nest logic
SELECT emp.name
FROM employee emp
JOIN employee emp_b
ON emp.name = emp_b.name;

--Implicit join, which support since Hive 0.13.0
SELECT emp.name, emph.sin_number
FROM employee emp, employee_hr emph
WHERE emp.name = emph.name;

--Join using different columns will create additional mapreduce
SELECT emp.name, empi.employee_id, emph.sin_number
FROM employee emp
JOIN employee_hr emph ON emp.name = emph.name
JOIN employee_id empi ON emph.employee_id = empi.employee_id;

--Streaming tables 
SELECT /*+ STREAMTABLE(employee_hr) */
emp.name, empi.employee_id, emph.sin_number
FROM employee emp
JOIN employee_hr emph ON emp.name = emph.name
JOIN employee_id empi ON emph.employee_id = empi.employee_id;

--Left JOIN
SELECT emp.name, emph.sin_number
FROM employee emp
LEFT JOIN employee_hr emph ON emp.name = emph.name;

--Right JOIN
SELECT emp.name, emph.sin_number
FROM employee emp
RIGHT JOIN employee_hr emph ON emp.name = emph.name;

--Full OUTER JOIN
SELECT emp.name, emph.sin_number
FROM employee emp
FULL JOIN employee_hr emph ON emp.name = emph.name;

--CROSS JOIN in different ways
SELECT emp.name, emph.sin_number
FROM employee emp
CROSS JOIN employee_hr emph;

SELECT emp.name, emph.sin_number
FROM employee emp
JOIN employee_hr emph;

SELECT emp.name, emph.sin_number
FROM employee emp
JOIN employee_hr emph on 1=1;

--unequal JOIN
SELECT emp.name, emph.sin_number
FROM employee emp
CROSS JOIN employee_hr emph WHERE emp.name <> emph.name;

--An example MAP JOIN enabled by query hint
SELECT /*+ MAPJOIN(employee) */ emp.name, emph.sin_number
FROM employee emp
CROSS JOIN employee_hr emph WHERE emp.name <> emph.name;

--BUCKET Map Join settings
SET hive.optimize.bucketmapjoin = true; 
SET hive.optimize.bucketmapjoin.sortedmerge = true;
SET hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat; 

--LEFT SEMI JOIN
SELECT a.name
FROM employee a
WHERE EXISTS
(SELECT * FROM employee_id b
WHERE a.name = b.name);

SELECT a.name
FROM employee a
LEFT SEMI JOIN employee_id b
ON a.name = b.name;

--UNION ALL including duplications
SELECT a.name as nm
FROM employee a
UNION ALL
SELECT b.name as nm
FROM employee_hr b;

--UNION
SELECT a.name as nm
FROM employee a
UNION
SELECT b.name as nm
FROM employee_hr b;

--Order with UNION
SELECT a.name as nm FROM employee a
UNION ALL
SELECT b.name as nm FROM employee_hr b
ORDER BY nm;

--Table employee implements INTERCEPT employee_hr
SELECT a.name 
FROM employee a
JOIN employee_hr b
ON a.name = b.name;

--Table employee implements MINUS employee_hr
SELECT a.name 
FROM employee a
LEFT JOIN employee_hr b
ON a.name = b.name
WHERE b.name IS NULL;

--Create partition table DDL.
--Load local data to table
LOAD DATA LOCAL INPATH '/home/dayongd/Downloads/employee_hr.txt' OVERWRITE INTO TABLE employee_hr;

--Load local data to partition table
LOAD DATA LOCAL INPATH '/home/dayongd/Downloads/employee.txt'
OVERWRITE INTO TABLE employee_partitioned
PARTITION (year=2018, month=12);

--Load HDFS data to table using default system path
LOAD DATA INPATH '/tmp/hivedemo/data/employee.txt' 
OVERWRITE INTO TABLE employee;

--Load HDFS data to table with full URI
LOAD DATA INPATH 
'hdfs://[dfs_hostname]:9000/tmp/hivedemo/data/employee.txt' 
OVERWRITE INTO TABLE employee;

--Data Exchange - INSERT
--Check the target table
SELECT name, work_place, gender_age FROM employee;

--Populate data from SELECT
INSERT INTO TABLE employee
SELECT * FROM ctas_employee;

--Verify the data loaded
SELECT name, work_place, gender_age FROM employee;

--Insert specified columns
CREATE TABLE emp_simple( -- Create a test table only has primary types
name string,
work_place string
);
INSERT INTO TABLE emp_simple(name) -- Specify which columns to insert
SELECT name FROM employee WHERE name = 'Will';

--Insert values
INSERT INTO TABLE emp_simple VALUES ('Michael', 'Toronto'),('Lucy', 'Montreal');
SELECT * FROM emp_simple;

--INSERT from CTE
WITH a as (SELECT * FROM ctas_employee )
FROM a
INSERT OVERWRITE TABLE employee
SELECT *;

--Multiple INSERTS by only scanning the source table once
FROM ctas_employee
INSERT OVERWRITE TABLE employee
SELECT *
INSERT OVERWRITE TABLE employee_internal
SELECT * 
INSERT OVERWRITE TABLE employee_partitioned partition(year=2018, month=9)
SELECT * 
;

--Dynamic partition is not enabled by default. We need to set following to make it work.
SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nostrict;

--Dynamic partition insert
INSERT INTO TABLE employee_partitioned PARTITION(year, month)
SELECT name, array('Toronto') as work_place, 
named_struct("gender","Male","age",30) as gender_age,
map("Python",90) as skills_score,
map("R&D",array('Developer')) as depart_title, 
year(start_date) as year, month(start_date) as month
FROM employee_hr eh
WHERE eh.employee_id = 102;

--Verify the inserted row
SELECT name,depart_title,year,month FROM employee_partitioned
WHERE name = 'Steven';

--Insert to local files with default row separators
INSERT OVERWRITE LOCAL DIRECTORY '/tmp/output1' 
SELECT * FROM employee;

--Insert to local files with specified row separators
INSERT OVERWRITE LOCAL DIRECTORY '/tmp/output2' 
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
SELECT * FROM employee;

--Multiple INSERT
FROM employee
INSERT OVERWRITE DIRECTORY '/tmp/output3'
SELECT *
INSERT OVERWRITE DIRECTORY '/tmp/output4'
SELECT * ;

--Export data and metadata of table
EXPORT TABLE employee TO '/tmp/output5';

--dfs -ls -R /tmp/output5/

--Import table with the same name
IMPORT FROM '/tmp/output5';              

--Import as new table
IMPORT TABLE empolyee_imported FROM '/tmp/output5';

--Import as external table 
IMPORT EXTERNAL TABLE empolyee_imported_external 
FROM '/tmp/output5'
LOCATION '/tmp/output6' ; --Note, LOCATION property is optional.

--Export and import to partitions
EXPORT TABLE employee_partitioned partition 
(year=2018, month=12) TO '/tmp/output7';

IMPORT TABLE employee_partitioned_imported 
FROM '/tmp/output7';                     

--ORDER, SORT
SELECT name FROM employee ORDER BY name DESC;
SELECT * FROM emp_simple ORDER BY work_place NULL LAST;

--Use more than 1 reducer
SET mapred.reduce.tasks = 2;

SELECT name FROM employee SORT BY name DESC;   

--Use only 1 reducer
SET mapred.reduce.tasks = 1; 

SELECT name FROM employee SORT BY name DESC;   

--Distribute by
SELECT name, employee_id 
FROM employee_hr DISTRIBUTE BY employee_id ; 

--Used with SORT BY
SELECT name, start_date FROM employee_hr DISTRIBUTE BY start_date SORT BY name;

--Cluster by
SELECT name, employee_id FROM employee_hr CLUSTER BY name ;   

--Complex datatype function
SELECT 
size(work_place) AS array_size, 
size(skills_score) AS map_size, 
size(depart_title) AS complex_size, 
size(depart_title["Product"]) AS nest_size 
FROM employee;

SELECT size(null), size(array(null)), size(array());

--Arrary functions
SELECT array_contains(work_place, 'Toronto') AS is_Toronto, sort_array(work_place) AS sorted_array FROM employee;

--Date and time functions
SELECT to_date(from_unixtime(unix_timestamp())) AS currentdate;

--To compare the difference of two date.
SELECT (unix_timestamp('2018-01-21 18:00:00') - unix_timestamp('2018-01-10 11:00:00'))/60/60/24 AS daydiff;

--Get the file name form a Linux path
SELECT reverse(split(reverse('/home/user/employee.txt'),'/')[0]) AS linux_file_name;  

--collect set or list
SELECT 
collect_set(gender_age.gender) AS gender_set,
collect_list(gender_age.gender) AS gender_list
FROM employee;

--virtual columns
SELECT INPUT__FILE__NAME,BLOCK__OFFSET__INSIDE__FILE AS OFFSIDE FROM employee;

--Transactions
--Below configuration parameters must be set appropriately to turn on transaction support in Hive.
SET hive.support.concurrency = true;
SET hive.enforce.bucketing = true;
SET hive.exec.dynamic.partition.mode = nonstrict;
SET hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
SET hive.compactor.initiator.on = true;
SET hive.compactor.worker.threads = 1;

--create table support transaction
CREATE TABLE employee_trans (
emp_id int,
name string,
start_date date,
quit_date date,
quit_flag string
) 
CLUSTERED BY (emp_id) INTO 2 BUCKETS STORED AS ORC
TBLPROPERTIES ('transactional'='true');

--Populate data
INSERT INTO TABLE employee_trans VALUES 
(100, 'Michael', '2017-02-01', null, 'N'),
(101, 'Will', '2017-03-01', null, 'N'),
(102, 'Steven', '2018-01-01', null, 'N'),
(104, 'Lucy', '2017-10-01', null, 'N');

--Update
UPDATE employee_trans SET quit_date = current_date, quit_flag = 'Y' WHERE emp_id = 104;
SELECT quit_date, quit_flag FROM employee_trans WHERE emp_id = 104;

--Delete
DELETE FROM employee_trans WHERE emp_id = 104;
SELECT * FROM employee_trans WHERE emp_id = 104;

--Merge
--prepare another table
CREATE TABLE employee_update (
emp_id int,
name string,
start_date date,
quit_date date,
quit_flag string
);
-- Populate data
INSERT INTO TABLE employee_update VALUES 
(100, 'Michael', '2017-02-01', '2018-01-01', 'Y'), -- People quite
(102, 'Steven', '2018-01-02', null, 'N'), -- People has start_date update
(105, 'Lily', '2018-04-01', null, 'N') -- People newly started
;

-- Do a data merge from employee_update to employee_trans
MERGE INTO employee_trans as tar USING employee_update as src
ON tar.emp_id = src.emp_id
WHEN MATCHED and src.quit_flag <> 'Y' THEN UPDATE SET start_date = src.start_date
WHEN MATCHED and src.quit_flag = 'Y' THEN DELETE
WHEN NOT MATCHED THEN INSERT VALUES (src.emp_id, src.name, src.start_date, src.quit_date, src.quit_flag);

--Show avaliable transactions
SHOW TRANSACTIONS;

--Show locks
SHOW LOCKS;


--Aggregation without GROUP BY columns
SELECT count(*) as rowcnt1, count(1) AS rowcnt2 FROM employee;

--Aggregation with GROUP BY columns
SELECT gender_age.gender, count(*) AS row_cnt FROM employee
GROUP BY gender_age.gender;

--The column age is not in the group by columns, 
--FAILED: SemanticException [Error 10002]: Line 1:15 Invalid column reference 'age'
--SELECT gender_age.age, gender_age.gender, count(*) AS row_cnt
--FROM employee GROUP BY gender_age.gender;

--Multiple aggregate functions are called in the same SELECT
SELECT gender_age.gender, AVG(gender_age.age) AS avg_age,
count(*) AS row_cnt FROM employee GROUP BY gender_age.gender;

--Aggregate functions are used with CASE WHEN 
SELECT sum(CASE WHEN gender_age.gender = 'Male' THEN gender_age.age
ELSE 0 END)/count(CASE WHEN gender_age.gender = 'Male' THEN 1
ELSE NULL END) AS male_age_avg FROM employee;

--Aggregate functions are used with COALESCE and IF 
SELECT
sum(coalesce(gender_age.age,0)) AS age_sum,
sum(if(gender_age.gender = 'Female',gender_age.age,0))
AS female_age_sum FROM employee;

--Nested aggregate functions are not allowed
--FAILED: SemanticException [Error 10128]: Line 1:11 Not yet supported place for UDAF 'count'
--SELECT avg(count(*)) AS row_cnt FROM employee; 

--Aggregate functions cannot apply to null
--SELECT sum(null), avg(null); 

--Aggregation across columns with NULL value.
SELECT max(null), min(null), count(null);
---Prepare a table for testing
CREATE TABLE t (val1 int, val2 int);
INSERT INTO TABLE t VALUES (1, 2),(null,2),(2,3);
----Check the table rows 
SELECT * FROM t;
----The 2nd row (NULL, 2) are ignored when doing sum(val1+val2)
SELECT sum(val1), sum(val1+val2) FROM t;                   
SELECT sum(coalesce(val1,0)), sum(coalesce(val1,0)+val2) FROM t;

--Aggregate functions can be also used with DISTINCT keyword to do aggregation on unique values.
SELECT count(distinct gender_age.gender) AS gender_uni_cnt, count(distinct name) AS name_uni_cnt FROM employee;

--Use max/min struct
SELECT gender_age.gender,
max(struct(gender_age.age, name)).col1 as age,
max(struct(gender_age.age, name)).col2 as name
FROM employee
GROUP BY gender_age.gender;

--Trigger single reducer during the whole processing
SELECT count(distinct gender_age.gender) AS gender_uni_cnt FROM employee;

--Use subquery to select unique value before aggregations for better performance
SELECT count(*) AS gender_uni_cnt FROM (SELECT distinct gender_age.gender FROM employee) a;

--Grouping Set
SELECT 
name, 
start_date,
count(sin_number) as sin_cnt 
FROM employee_hr
GROUP BY name, start_date 
GROUPING SETS((name, start_date));
--||-- equals to
SELECT 
name, 
start_date, 
count(sin_number) AS sin_cnt 
FROM employee_hr
GROUP BY name, start_date;

SELECT 
name, start_date, count(sin_number) as sin_cnt 
FROM employee_hr
GROUP BY name, start_date 
GROUPING SETS(name, start_date);
--||-- equals to
SELECT 
name, null as start_date, count(sin_number) as sin_cnt 
FROM employee_hr
GROUP BY name
UNION ALL
SELECT 
null as name, start_date, count(sin_number) as sin_cnt 
FROM employee_hr
GROUP BY start_date;

SELECT 
name, start_date, count(sin_number) as sin_cnt 
FROM employee_hr
GROUP BY name, start_date 
GROUPING SETS((name, start_date), name);
--||-- equals to
SELECT 
name, start_date, count(sin_number) as sin_cnt 
FROM employee_hr
GROUP BY name, start_date
UNION ALL
SELECT 
name, null as start_date, count(sin_number) as sin_cnt 
FROM employee_hr
GROUP BY name;

SELECT 
name, start_date, count(sin_number) as sin_cnt 
FROM employee_hr
GROUP BY name, start_date 
GROUPING SETS((name, start_date), name, start_date, ());
--||-- equals to
SELECT 
name, start_date, count(sin_number) AS sin_cnt 
FROM employee_hr
GROUP BY name, start_date
UNION ALL
SELECT 
name, null as start_date, count(sin_number) AS sin_cnt 
FROM employee_hr
GROUP BY name
UNION ALL
SELECT 
null as name, start_date, count(sin_number) AS sin_cnt 
FROM employee_hr
GROUP BY start_date
UNION ALL
SELECT 
null as name, null as start_date, count(sin_number) AS sin_cnt 
FROM employee_hr

--GROUPING__ID and grouping
SELECT 
name, start_date, count(employee_id) as emp_id_cnt,
GROUPING__ID,
grouping(name) as gp_name, grouping(start_date) as gp_sd
FROM employee_hr 
GROUP BY name, start_date 
WITH CUBE ORDER BY name, start_date;

--Aggregation condition – HAVING
SELECT gender_age.age FROM employee GROUP BY gender_age.age HAVING count(*)=1;
SELECT gender_age.age, count(*) as cnt FROM employee GROUP BY gender_age.age HAVING cnt=1;

--If we do not use HAVING, we can use subquery as follows. 
SELECT a.age
FROM
(SELECT count(*) as cnt, gender_age.age
FROM employee GROUP BY gender_age.age
) a WHERE a.cnt<=1;

--Prepare table and data for demonstration
CREATE TABLE IF NOT EXISTS employee_contract
(
name string,
dept_num int,
employee_id int,
salary int,
type string,
start_date date
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA INPATH
'/tmp/hivedemo/data/employee_contract.txt' 
OVERWRITE INTO TABLE employee_contract;

--window aggregate functions
SELECT 
name, 
dept_num as deptno, 
salary,
count(*) OVER (PARTITION BY dept_num) as cnt,
count(distinct dept_num) OVER (PARTITION BY dept_num) as dcnt,
sum(salary) OVER(PARTITION BY dept_num ORDER BY dept_num) as sum1,
sum(salary) OVER(ORDER BY dept_num) as sum2,
sum(salary) OVER(ORDER BY dept_num, name) as sum3
FROM employee_contract
ORDER BY deptno, name;

--window sorting functions
SELECT 
name, 
dept_num as deptno, 
salary,
row_number() OVER () as rnum,
rank() OVER (PARTITION BY dept_num ORDER BY salary) as rk, 
dense_rank() OVER (PARTITION BY dept_num ORDER BY salary) as drk,
percent_rank() OVER(PARTITION BY dept_num ORDER BY salary) as prk,
ntile(4) OVER(PARTITION BY dept_num ORDER BY salary) as ntile
FROM employee_contract
ORDER BY deptno, name;

--aggregate in over clause
SELECT
dept_num,
rank() OVER (PARTITION BY dept_num ORDER BY sum(salary)) as rk
FROM employee_contract
GROUP BY dept_num;

--window analytics function
SELECT 
name,
dept_num as deptno,
salary,
round(cume_dist() OVER (PARTITION BY dept_num ORDER BY salary), 2) as cume,
lead(salary, 2) OVER (PARTITION BY dept_num ORDER BY salary) as lead,
lag(salary, 2, 0) OVER (PARTITION BY dept_num ORDER BY salary) as lag,
first_value(salary) OVER (PARTITION BY dept_num ORDER BY salary) as fval,
last_value(salary) OVER (PARTITION BY dept_num ORDER BY salary) as lvalue,
last_value(salary) OVER (PARTITION BY dept_num ORDER BY salary RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS lvalue2
FROM employee_contract 
ORDER BY deptno, salary;

--window expression preceding and following
SELECT 
name, dept_num as dno, salary AS sal,
max(salary) OVER (PARTITION BY dept_num ORDER BY name ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) win1,
max(salary) OVER (PARTITION BY dept_num ORDER BY name ROWS BETWEEN 2 PRECEDING AND UNBOUNDED FOLLOWING) win2,
max(salary) OVER (PARTITION BY dept_num ORDER BY name ROWS BETWEEN 1 PRECEDING AND 2 FOLLOWING) win3,
max(salary) OVER (PARTITION BY dept_num ORDER BY name ROWS BETWEEN 2 PRECEDING AND 1 PRECEDING) win4,
max(salary) OVER (PARTITION BY dept_num ORDER BY name ROWS BETWEEN 1 FOLLOWING AND 2 FOLLOWING) win5,
max(salary) OVER (PARTITION BY dept_num ORDER BY name ROWS 2 PRECEDING) win6,
max(salary) OVER (PARTITION BY dept_num ORDER BY name ROWS UNBOUNDED PRECEDING) win7
FROM employee_contract
ORDER BY dno, name;

--window expression current_row
SELECT 
name, dept_num as dno, salary AS sal,
max(salary) OVER (PARTITION BY dept_num ORDER BY name ROWS BETWEEN CURRENT ROW AND CURRENT ROW) win8,
max(salary) OVER (PARTITION BY dept_num ORDER BY name ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING) win9,
max(salary) OVER (PARTITION BY dept_num ORDER BY name ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) win10,
max(salary) OVER (PARTITION BY dept_num ORDER BY name ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) win11,
max(salary) OVER (PARTITION BY dept_num ORDER BY name ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) win12,
max(salary) OVER (PARTITION BY dept_num ORDER BY name ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING) win13,
max(salary) OVER (PARTITION BY dept_num ORDER BY name ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) win14
FROM employee_contract
ORDER BY dno, name;

--window reference
SELECT name, dept_num, salary,
MAX(salary) OVER w1 AS win1,
MAX(salary) OVER w2 AS win2,
MAX(salary) OVER w3 AS win3
FROM employee_contract
WINDOW
w1 as (PARTITION BY dept_num ORDER BY name ROWS BETWEEN 2 PRECEDING AND CURRENT ROW),
w2 as w3,
w3 as (PARTITION BY dept_num ORDER BY name ROWS BETWEEN 1 PRECEDING AND 2 FOLLOWING)
;

--window with range type
SELECT 
dept_num, start_date, name, salary, 
max(salary) OVER (PARTITION BY dept_num ORDER BY salary
RANGE BETWEEN 500 PRECEDING AND 1000 FOLLOWING) win1,
max(salary) OVER (PARTITION BY dept_num ORDER BY salary
RANGE BETWEEN 500 PRECEDING AND CURRENT ROW) win2
FROM employee_contract
order by dept_num, start_date;

--random sampling
SELECT name FROM employee_hr DISTRIBUTE BY rand() SORT BY rand() LIMIT 2;

--Bucket table sampling example
--based on whole row
SELECT name FROM employee_trans TABLESAMPLE(BUCKET 1 OUT OF 2 ON rand()) a;
--based on bucket column
SELECT name FROM employee_trans TABLESAMPLE(BUCKET 1 OUT OF 2 ON emp_id) a;

--Block sampling - Sample by rows
SELECT name FROM employee TABLESAMPLE(1 ROWS) a;

--Sample by percentage of data size
SELECT name FROM employee TABLESAMPLE(50 PERCENT) a;

--Sample by data size
SELECT name FROM employee TABLESAMPLE(3b) a;   


--Query explain
EXPLAIN SELECT gender_age.gender, count(*) FROM employee_partitioned WHERE year=2018 GROUP BY gender_age.gender LIMIT 2;

--ANALYZE statement
ANALYZE TABLE employee COMPUTE STATISTICS;

ANALYZE TABLE employee COMPUTE STATISTICS NOSCAN;

ANALYZE TABLE employee_partitioned PARTITION(year=2018, month=12) COMPUTE STATISTICS;

ANALYZE TABLE employee_partitioned PARTITION(year, month) COMPUTE STATISTICS;

ANALYZE TABLE employee_id COMPUTE STATISTICS FOR COLUMNS employee_id;           

--Check the statistics  
DESCRIBE EXTENDED employee_partitioned PARTITION(year=2018, month=12);

DESCRIBE EXTENDED employee;

DESCRIBE FORMATTED employee.name;

SET hive.stats.autogather=ture;

--Create Index
CREATE INDEX idx_id_employee_id
ON TABLE employee_id (employee_id)
AS 'COMPACT'
WITH DEFERRED REBUILD;

CREATE INDEX idx_gender_employee_id
ON TABLE employee_id (gender_age)
AS 'BITMAP'
WITH DEFERRED REBUILD;

--Rebuild index
ALTER INDEX idx_id_employee_id ON employee_id REBUILD;
ALTER INDEX idx_gender_employee_id ON employee_id REBUILD;

--show index tables
SHOW TABLES '*idx*';

--Show index
DESC default__employee_id_idx_id_employee_id__;
SELECT * FROM default__employee_id_idx_id_employee_id__;

--Drop index
DROP INDEX idx_id_employee_id ON employee_id;
DROP INDEX idx_gender_employee_id ON employee_id;

--Use screw tables
CREATE TABLE sample_skewed_table (
dept_no int, 
dept_name string
) 
SKEWED BY (dept_no) ON (1000, 2000);

DESC FORMATTED sample_skewed_table;

--Data file optimization
--File format
SET hive.exec.compress.output=true; 
SET io.seqfile.compression.type=BLOCK; 

--Compression
SET hive.exec.compress.intermediate=true;
SET hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
SET hive.exec.compress.output=true;
SET mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec

--Storage optimization
SET hive.exec.mode.local.auto=true;
SET hive.exec.mode.local.auto.inputbytes.max=50000000;
SET hive.exec.mode.local.auto.input.files.max=5; 

--JVM reuse
SET mapreduce.job.jvm.numtasks=5;

--Parallel running job
SET hive.exec.parallel=true; 
SET hive.exec.parallel.thread.number=16; 

--Map Join
SET hive.auto.convert.join=true; 
SET hive.mapjoin.smalltable.filesize=600000000; 
SET hive.auto.convert.join.noconditionaltask = true; 
SET hive.auto.convert.join.noconditionaltask.size = 10000000;
 
--Bucket Map Join
SET hive.auto.convert.join=true; 
SET hive.optimize.bucketmapjoin=true; 

--Sort Merge Bucket (SMB) Join
SET hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
SET hive.auto.convert.sortmerge.join=true;
SET hive.optimize.bucketmapjoin=true;
SET hive.optimize.bucketmapjoin.sortedmerge=true;
SET hive.auto.convert.sortmerge.join.noconditionaltask=true;

--Sort Merge Bucket Map Join
SET hive.auto.convert.join=true;
SET hive.auto.convert.sortmerge.join=true;
SET hive.optimize.bucketmapjoin=true;
SET hive.optimize.bucketmapjoin.sortedmerge=true;
SET hive.auto.convert.sortmerge.join.noconditionaltask=true;
SET hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;
 
--Skew Join
SET hive.optimize.skewjoin=true; 
SET hive.skewjoin.key=100000; 

--Skew data in GROUP BY 
SET hive.groupby.skewindata=true; 

--Job engine
--vectorized
SET hive.vectorized.execution.enabled=true; -- default false
--cbo
SET hive.cbo.enable=true; -- default true after v0.14.0
SET hive.compute.query.using.stats=true; -- default false
SET hive.stats.fetch.column.stats=true; -- default false
SET hive.stats.fetch.partition.stats=true; -- default true





--UDF deployment 
CREATE TEMPORARY FUNCTION tmptoUpper 
as 'com.packtpub.hive.essentials.hiveudf.toupper';
USING JAR 'hdfs:///app/hive/function/hiveudf-1.0.jar';

CREATE FUNCTION toUpper
as 'hive.essentials.hiveudf.ToUpper' 
USING JAR 'hdfs:///app/hive/function/hiveudf-1.0.jar';

SHOW FUNCTIONS ToUpper;
DESCRIBE FUNCTION ToUpper;
DESCRIBE FUNCTION EXTENDED ToUpper;

RELOAD FUNCTION;

SELECT name, toUpper(name) as cap_name, tmptoUpper(name) as cname FROM employee;

DROP TEMPORARY FUNCTION IF EXISTS tmptoUpper;
DROP FUNCTION IF EXISTS toUpper;
 
--Streaming, call the script in Hive CLI from HQL.
ADD FILE /tmp/upper.py;
SELECT TRANSFORM (name,work_place[0]) 
USING 'python upper.py' AS (CAP_NAME,CAP_PLACE) 
FROM employee;

--LazySimpleSerDe
CREATE TABLE test_serde_lz
STORED as TEXTFILE as
SELECT name from employee;

--ColumnarSerDe
CREATE TABLE test_serde_rc
STORED as RCFile as
SELECT name from employee;

--ColumnarSerDe
CREATE TABLE test_serde_orc
STORED as ORC as
SELECT name from employee;

--RegexSerDe-Parse , seperate fields
CREATE TABLE test_serde_rex(
name string,
gender string,
age string
)
ROW FORMAT SERDE
'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
WITH SERDEPROPERTIES(
'input.regex' = '([^,]*),([^,]*),([^,]*)',
'output.format.string' = '%1$s %2$s %3$s'
)
STORED as TEXTFILE;

--HBaseSerDe. Make sure you have HBase installed before running query below.
CREATE TABLE test_serde_hb(
id string,
name string,
gender string,
age string
)
ROW FORMAT SERDE
'org.apache.hadoop.hive.hbase.HBaseSerDe'
STORED BY
'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
"hbase.columns.mapping"=
":key,info:name,info:gender,info:age"
)
TBLPROPERTIES("hbase.table.name" = "test_serde");

--AvroSerDe 3 ways
CREATE TABLE test_serde_avro( -- Specify schema directly 
name string,
gender string,
age string
)
STORED as AVRO;

CREATE TABLE test_serde_avro2 -- Specify schema from properties 
STORED as AVRO
TBLPROPERTIES (
  'avro.schema.literal'='{
   "type":"record",
   "name":"user",
   "fields":[ 
   {"name":"name", "type":"string"}, 
   {"name":"gender", "type":"string", "aliases":["gender"]},
   {"name":"age", "type":"string", "default":"null"}
   ]
  }'
);

-- Using schema file below is more flexiable
CREATE TABLE test_serde_avro3 -- Specify schema from schema file 
STORED as AVRO
TBLPROPERTIES (
'avro.schema.url'='/tmp/schema/test_avro_schema.avsc'
);

--ParquetHiveSerDe
CREATE TABLE test_serde_parquet
STORED as PARQUET as
SELECT name from employee;

--OpenCSVSerDe
CREATE TABLE test_serde_csv(
name string,
gender string,
age string
)
ROW FORMAT SERDE
'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
  "separatorChar" = "\t",
  "quoteChar" = "'",
  "escapeChar" = "\\"
) 
STORED as TEXTFILE;

--JSONSerDe
CREATE TABLE test_serde_js(
name string,
gender string,
age string
)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
STORED as TEXTFILE;

--Use hash function
SELECT 
name, 
md5(name) as md5_name, -- 128 bit
sha1(name) as sha1_name, -- 160 bit
sha2(name, 256) as sha2_name -- 256 bit
FROM employee;

--Use data mask udf
select 
mask("Card-0123-4567-8910", "U", "l", "#") as m0, 
mask_first_n("Card-0123-4567-8910", 4) as m1, 
mask_last_n("Card-0123-4567-8910", 4) as m2,
mask_show_first_n("Card-0123-4567-8910", 4) as m3,
mask_show_last_n("Card-0123-4567-8910", 4) as m4,
mask_hash('Card-0123-4567-8910') as m5;

--Use built-in UDF for encryption/decryption
SELECT
name,
aes_encrypt(name,'1234567890123456') as encrypted,
aes_decrypt(
aes_encrypt(name,'1234567890123456'),
'1234567890123456') as decrypted
FROM employee;

--Use encryption and decryption as 3rd party UDF 
ADD JAR /home/dayongd/Downloads/hiveessentials-1.0-SNAPSHOT.jar;                    

CREATE TEMPORARY FUNCTION aesencrypt AS 'com.packtpub.hive.essentials.hiveudf.AESEncrypt';
CREATE TEMPORARY FUNCTION aesdecrypt AS 'com.packtpub.hive.essentials.hiveudf.AESDecrypt';

SELECT aesencrypt('Will') AS encrypt_name FROM employee LIMIT 1;                         
SELECT aesdecrypt('YGvo54QIahpb+CVOwv9OkQ==') AS decrypt_name FROM employee LIMIT 1;  


--Hive HBase integration
CREATE EXTERNAL TABLE hbase_table_sample(
id int,
value1 string,
value2 string,
map_value map<string, string>
)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf1:val,cf2:val,cf3:")
TBLPROPERTIES ("hbase.table.name" = "table_name_in_hbase");

--Hive Mongo integration
ADD JAR mongo-hadoop-core-2.0.2.jar;
CREATE EXTERNAL TABLE mongodb_table_sample(
id int,
value1 string,
value2 string
)
STORED BY 'com.mongodb.hadoop.hive.MongoStorageHandler'
WITH SERDEPROPERTIES (
'mongo.columns.mapping'='{"id":"_id","value1":"value1","value2":"value2"}')
TBLPROPERTIES(
'mongo.uri'='mongodb://localhost:27017/default.mongo_sample'
);







